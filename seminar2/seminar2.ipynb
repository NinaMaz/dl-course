{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano and Lasagne\n",
    "\n",
    "\n",
    "### Installation instructions\n",
    "\n",
    "Istall via pip by executing the following commands:\n",
    "\n",
    "<code>pip install --upgrade https://github.com/Theano/Theano/archive/master.zip\n",
    "pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip<\\code>\n",
    "\n",
    "or go to this page for further clarifications:\n",
    "\n",
    "http://lasagne.readthedocs.org/en/latest/user/installation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warming up: compare timings\n",
    "* Implement a function that computes the sum of squares of numbers from 0 to N\n",
    "* Use numpy or python\n",
    "* An array of numbers 0 to N - numpy.arange(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#no numpy, just python\n",
    "def sum_squares(N):\n",
    "    return sum([x**2 for x in range(N)])\n",
    "\n",
    "%timeit -n 1 sum_squares(10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the same computation with numpy\n",
    "import numpy as np\n",
    "def sum_squares(N):\n",
    "    return (np.arange(N)**2).sum()\n",
    "\n",
    "%timeit -n 1 sum_squares(10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the same computation with theano\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "#input variable\n",
    "N = T.scalar(\"number of terms\", dtype='int32')\n",
    "\n",
    "#a recipe how to produce sum of squares of arange of N given N\n",
    "result = (T.arange(N)**2).sum()\n",
    "\n",
    "#compilation of function\n",
    "sum_squares = theano.function(inputs = [N],outputs=result)\n",
    "\n",
    "%timeit -n 1 sum_squares(10**6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One\n",
    "# Theano: key concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Symbolic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eps = T.scalar('example scalar', dtype='float32')\n",
    "v = T.vector()#dtype = theano.config.floatX by default\n",
    "x = T.tensor4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transformations\n",
    "two_times_v = 2 * v\n",
    "elemetwise_sin = T.sin(v)\n",
    "x_normalized = (x - x.mean(axis=0)) / (x.std(axis=0) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.printing.debugprint(two_times_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Functions\n",
    "\n",
    "With variables and expressions, you can define a recipe for computation, but you need a function to actually compute something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#symbolic variables are placeholders for input values in our function\n",
    "a = T.vector('first input')\n",
    "b = T.vector('second input')\n",
    "\n",
    "#theano expressions are a scheme of computation process\n",
    "c = a + b\n",
    "\n",
    "inputs = [a, b]\n",
    "outputs = c\n",
    "\n",
    "#we compile a function\n",
    "#theano requires function inputs to be in a list, evem if the function has single input\n",
    "f = theano.function(inputs, outputs)\n",
    "\n",
    "#now we can perform an actual computation\n",
    "print(f(np.ones(5), np.ones(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequent problem: numpy uses float64 by default, theano - float32.\n",
    "Feeding float64 inputs to the function compiled for float32 will produce an error.\n",
    "There are several ways to overcome this problem\n",
    "* make numpy to use float32 with <code>dtype=np.float32</code>, or explicitly convert your data via <code>.astype(np.float32) </code>\n",
    "* set type of your input variables to 'float64'\n",
    "* compile function with flag <code>allow_input_downcast=True</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Shared variables\n",
    "\n",
    "* the inputs and transformations only exist when function is called\n",
    "\n",
    "* shared variables always stay in memory, and can be shared between multiple functions\n",
    "\n",
    "* hybrid symbolic and non-symbolic variables\n",
    " \n",
    "* a perfect place to store network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating shared variable\n",
    "shared_vector = theano.shared(np.ones(5))\n",
    "\n",
    "#shared variable looks like a symbolic variable\n",
    "print(shared_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#but it can be evaluated because it's got a value inside\n",
    "print(shared_vector.get_value())\n",
    "\n",
    "#within symbolic graph you use them just as any other inout or transformation, no \"get value\" needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#setting new value\n",
    "shared_vector.set_value(np.arange(4))\n",
    "\n",
    "print(shared_vector.get_value())\n",
    "\n",
    "#Note that the vector changed shape\n",
    "#This is entirely allowed... unless your graph is hard-wired to work with some fixed shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Updates\n",
    "\n",
    "* updates are a way to change shared variable value after a function call\n",
    "\n",
    "* update is a dictionary of a form {shared_variable : an expression for new value} which is has to be provided when function is compiled\n",
    "\n",
    "That's how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Accumulator function\n",
    "input_scalar = T.scalar(name='input',dtype='float64')\n",
    "accum = theano.shared(0.)\n",
    "\n",
    "inputs = [input_scalar]\n",
    "outputs = []\n",
    "my_updates = {accum: accum + input_scalar}\n",
    "\n",
    "f = theano.function(inputs, outputs, updates=my_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print accum.get_value()\n",
    "f(1)\n",
    "print accum.get_value()\n",
    "f(1)\n",
    "print accum.get_value()\n",
    "f(137)\n",
    "print accum.get_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Special feature: symbolic differentiation\n",
    "* Theano can compute derivatives and gradients automatically\n",
    "* Derivatives are computed symbolically, not numerically\n",
    "\n",
    "Limitations:\n",
    "* You can only compute a gradient of a __scalar__ over one or several scalars,  vector or tensors\n",
    "* A transformation has to have float32 or float64 dtype throughout the whole computation graph\n",
    " * derivative over an integer has no mathematical sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_scalar = T.scalar(name='input',dtype='float64')\n",
    "\n",
    "scalar_squared = T.sum(my_scalar**2)\n",
    "\n",
    "#a derivative of v_squared by my_vector\n",
    "derivative = T.grad(scalar_squared, my_scalar)\n",
    "\n",
    "f = theano.function([my_scalar], scalar_squared)\n",
    "grad = theano.function([my_scalar], derivative) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.linspace(-3, 3)\n",
    "x_squared = list(map(f, x))\n",
    "d_x_squared = list(map(grad,x))\n",
    "\n",
    "plt.plot(x, x_squared, label=\"$x^2$\")\n",
    "plt.plot(x, d_x_squared, label=\"derivative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_vector = T.vector('float64')\n",
    "\n",
    "#Compute the gradient of the next weird function over my_scalar and my_vector\n",
    "#warning! Trying to understand the meaning of that function may result in permanent brain damage\n",
    "\n",
    "weird_psychotic_function = ((my_vector+my_scalar)**(1+T.var(my_vector)) +1./T.arcsinh(my_scalar)).mean()/(my_scalar**2 +1) + 0.01*T.sin(2*my_scalar**1.5)*(T.sum(my_vector)* my_scalar**2)*T.exp((my_scalar-4)**2)/(1+T.exp((my_scalar-4)**2))*(1.-(T.exp(-(my_scalar-4)**2))/(1+T.exp(-(my_scalar-4)**2)))**2\n",
    "\n",
    "\n",
    "der_by_scalar,der_by_vector = <student.compute_grad_over_scalar_and_vector()>\n",
    "\n",
    "\n",
    "compute_weird_function = theano.function([my_scalar,my_vector],weird_psychotic_function)\n",
    "compute_der_by_scalar = theano.function([my_scalar,my_vector],der_by_scalar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plotting your derivative\n",
    "vector_0 = [1,2,3]\n",
    "\n",
    "scalar_space = np.linspace(0,7)\n",
    "\n",
    "y = [compute_weird_function(x,vector_0) for x in scalar_space]\n",
    "plt.plot(scalar_space,y,label='function')\n",
    "y_der_by_scalar = [compute_der_by_scalar(x,vector_0) for x in scalar_space]\n",
    "plt.plot(scalar_space,y_der_by_scalar,label='derivative')\n",
    "plt.grid();plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: logistic regression\n",
    "\n",
    "Implement the regular logistic regression training algorithm\n",
    "\n",
    "Tips:\n",
    "* Weights fit in as a shared variable\n",
    "* X and y are potential inputs\n",
    "* Compile 2 functions:\n",
    " * train_function(X,y) - returns error and computes weights' new values __(through updates)__\n",
    " * predict_fun(X) - just computes probabilities (\"y\") given data\n",
    " \n",
    " \n",
    "We shall train on a two-class MNIST dataset\n",
    "* please note that target y are {0,1} and not {-1,1} as in some formulae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "mnist = load_digits(2)\n",
    "\n",
    "X,y = mnist.data, mnist.target\n",
    "\n",
    "\n",
    "print (\"y [shape - %s]:\"%(str(y.shape)),y[:10])\n",
    "\n",
    "print (\"X [shape - %s]:\"%(str(X.shape)))\n",
    "print (X[:3])\n",
    "print (y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inputs and shareds\n",
    "shared_weights = <student.code_me()>\n",
    "input_X = <student.code_me()>\n",
    "input_y = <student.code_me()>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_y = <predicted probabilities for input_X>\n",
    "\n",
    "\n",
    "loss = <logistic loss (scalar, mean over sample)>\n",
    "\n",
    "\n",
    "grad = <gradient of loss over model weights>\n",
    "\n",
    "\n",
    "updates = {\n",
    "    shared_weights: <new weights after gradient step>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_function = <compile function that takes X and y, returns log loss and updates weights>\n",
    "\n",
    "predict_function = <compile function that takes X and computes probabilities of y>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for i in range(5):\n",
    "    loss_i = train_function(X_train,y_train)\n",
    "    print (\"loss at iter %i:%.4f\"%(i,loss_i))\n",
    "    print (\"train auc:\",roc_auc_score(y_train,predict_function(X_train)))\n",
    "    print (\"test auc:\",roc_auc_score(y_test,predict_function(X_test)))\n",
    "\n",
    "    \n",
    "print (\"resulting weights:\")\n",
    "plt.imshow(shared_weights.get_value().reshape(8,-1))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Two: Lasagne\n",
    "* lasagne is a library for neural network building and training, built on top of theano\n",
    "\n",
    "For a demo we shall solve the same MNIST digit recognition problem\n",
    "* image size: 28x28\n",
    "* 10 different digits\n",
    "* 50k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mnist import load_dataset\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "\n",
    "print X_train.shape, y_train.shape\n",
    "print X_val.shape, y_val.shape\n",
    "print X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "\n",
    "X = T.tensor4()\n",
    "y = T.vector(dtype='int32')\n",
    "\n",
    "input_layer = lasagne.layers.InputLayer(shape=[None, 1, 28, 28], input_var=X)\n",
    "#None means arbitrary number and only works for the first axis\n",
    "#if you don't create input variable, input layer will create it automatically\n",
    "\n",
    "#fully connected layer, that takes input layer and applies 50 neurons to it.\n",
    "# nonlinearity here is sigmoid as in logistic regression\n",
    "# you can give a name to each layer (optional)\n",
    "dense_1 = lasagne.layers.DenseLayer(input_layer,num_units=50,\n",
    "                                   nonlinearity = lasagne.nonlinearities.sigmoid,\n",
    "                                   name = \"hidden_layer\")\n",
    "\n",
    "#fully connected output layer that takes dense_1 as input and has 10 neurons (1 for each digit)\n",
    "#We use softmax nonlinearity to make probabilities add up to 1\n",
    "output_layer = lasagne.layers.DenseLayer(dense_1, num_units = 10,\n",
    "                                        nonlinearity = lasagne.nonlinearities.softmax,\n",
    "                                        name='output_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#network prediction (theano-transformation)\n",
    "prediction = lasagne.layers.get_output(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all network weights (shared variables)\n",
    "weights = lasagne.layers.get_all_params(output_layer)\n",
    "print (weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, we can go the hard way, and do everyting with theano only\n",
    "* define loss function manually\n",
    "* compute error gradient over all weights\n",
    "* define updates\n",
    "* But that's a lot of work and life's short\n",
    "\n",
    "Instead, we shall use Lasagne builtins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mean categorical crossentropy as a loss function - similar to logistic loss but for multiclass targets\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, y).mean()\n",
    "\n",
    "#prediction accuracy\n",
    "accuracy = lasagne.objectives.categorical_accuracy(prediction, y).mean()\n",
    "\n",
    "#This function computes gradient AND composes weight updates just like you did earlier\n",
    "updates_sgd = lasagne.updates.sgd(loss, weights, learning_rate=0.01)\n",
    "\n",
    "#function that computes loss and updates weights\n",
    "train_fn = theano.function([X, y], [loss, accuracy], updates=updates_sgd)\n",
    "\n",
    "#function that just computes accuracy\n",
    "accuracy_fn = theano.function([X, y], accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's all, now let's train it!\n",
    "* We got a lot of data, so it's recommended that you use SGD\n",
    "* So let's implement a function that splits the training sample into minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# An auxilary function that returns mini-batches for neural network training\n",
    "\n",
    "#Parameters\n",
    "# X - a tensor of images with shape (many, 1, 28, 28), e.g. X_train\n",
    "# y - a vector of answers for corresponding images e.g. Y_train\n",
    "#batch_size - a single number - the intended size of each batches\n",
    "\n",
    "#What do need to implement\n",
    "# 1) Shuffle data\n",
    "# - You have to shuffle X and y the same way, or the correspondence between X_i and y_i will be broken\n",
    "# 3) Split data into minibatches of batch_size\n",
    "# - If data size is not a multiple of batch_size, make one last batch smaller.\n",
    "# 4) return a list (or an iterator) of pairs\n",
    "# - (images, corresponding labels)\n",
    "\n",
    "def iterate_minibatches(X, y, batch_size):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 100 #amount of passes through the data\n",
    "\n",
    "batch_size = 50 #number of samples processed at each function call\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch = train_fn(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fn(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fn(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 99:\n",
    "    print (\"Achievement unlocked: 80lvl Warlock!\")\n",
    "else:\n",
    "    print (\"We need more magic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: make a better network\n",
    "\n",
    "\n",
    "* The quest is to create a network that gets at least 99% at test set\n",
    "\n",
    "## Tips on what can be done:\n",
    "\n",
    "   * more layers!\n",
    "   * more neurons!  \n",
    "   * ok, forget about more neurons. If you use fully-connected layers, it will certainly lead to overfitting. Use convolutions instead\n",
    "   * `network = lasagne.layers.Conv2DLayer(prev_layer,`\n",
    "    `                       num_filters = n_neurons,`\n",
    "    `                        filter_size = (filter width, filter height),`\n",
    "    `                        nonlinearity = some_nonlinearity)`\n",
    "   * Warning! Training convolutional networks can take long without GPU\n",
    "   \n",
    " * Regularize to prevent overfitting\n",
    "   * Add some L2 weight norm to the loss function, theano will do the rest\n",
    "   * Can be done manually or via - http://lasagne.readthedocs.org/en/latest/modules/regularization.html\n",
    "   \n",
    " * Better optimization method - rmsprop, nesterov_momentum, adadelta, adagrad and so on.\n",
    "   * Converge faster and sometimes reach better optima\n",
    "   * It might make sense to tweak learning rate, other learning parameters, batch size and number of epochs   \n",
    "   \n",
    " * Dropout - to prevent overfitting\n",
    "   * `lasagne.layers.DropoutLayer(prev_layer, p=probability_to_zero_out)`\n",
    "   \n",
    " * Plenty other layers and architectures\n",
    "   * http://lasagne.readthedocs.org/en/latest/modules/layers.html\n",
    "   * batch normalization, pooling, etc\n",
    "   \n",
    " * Nonlinearities in the hidden layers\n",
    "   * tanh, relu, leaky relu, etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
